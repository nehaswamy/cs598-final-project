{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Link to Video\n"
      ],
      "metadata": {
        "id": "aS0kIeESkiia"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Introduction\n",
        "This project is based on \"Back to the Basics with Inclusion of Clinical Domain Knowledge - A Simple, Scalable and Effective Model of Alzheimer’s Disease Classification\" (Sarah C. Brüningk, Felix Hensel, Louis P. Lukas, Merel Kuijs, Catherine R. Jutzeler, Bastian Rieck Proceedings of the 6th Machine Learning for Healthcare Conference, PMLR 149:730-754, 2021.)\n",
        "\n",
        "The authors of the original paper analyze MRIs of patients with Alzheimer's disease to detect the onset of the disease early, predict disease progression, and optimize treatment plans for patients.\n",
        "\n",
        "Currently, there is a push for more complex machine learning models, like deep CNNs with unsupervised learning and ensemble models that combine different architectures. Complex models may perform well, but they are not necessarily applicable in clinical settings because high-resolution MRIs lead to memory and hardware issues. One method used to achieve high performance is down-sampling data, but this means that high-resolution data that could be useful is scrapped.\n",
        "\n",
        "The authors hypothesize that using simple models that include clinical domain knowledge of Alzheimer's disease and its topological features will result in a classification performance similar to that of complex models while also being faster, more scalable, and more interpretable. They evaluated the performance of CNNs on subsets of full-resolution three-dimensional MRIs. Then, they performed an ablation study to compare different levels of biological scale and anatomical brain areas. Finally, they used topological data analysis (TDA) to analyze changes in whole brain tissue connectivity.\n",
        "\n",
        "They found that choosing a meaningful data representation consisting of the left hippocampus allowed them to achieve better performance compared to more complex architectures with many parameters. In addition, they were able to save on computational cost and hardware requirements. This means that clinical domain knowledge may be more important than architecture design and complexity, specifically for Alzheimer's Disease classification."
      ],
      "metadata": {
        "id": "MQ0sNuMePBXx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Scope of Reproducibility\n",
        "\n",
        "The authors' hypotheses were\n",
        "1. \"...using simple model architectures that include prior domain knowledge of morphological hallmarks of [Alzheimer's Disease] and topological feature thereof, will result in classification performance comparable to more complex model[s]\" (3), and\n",
        "2. \"...[their] models will facilitate fast, scalable, and interpretable solutions through meaningful representations\" (3).\n",
        "\n",
        "In other words, they sought to determine if simple models are faster, more usable, and more understandable than highly complex models with too many hyperparameters.\n",
        "\n",
        "In this project, I use a simple CNN model, using HW 3: CNNs as a guide, to test if a basic CNN will have a high accuracy (>= 70%) and perform fast.\n"
      ],
      "metadata": {
        "id": "uygL9tTPSVHB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Methodology"
      ],
      "metadata": {
        "id": "xWAHJ_1CdtaA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Environment"
      ],
      "metadata": {
        "id": "s4vHh1Da5SAo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Python version: 3.10"
      ],
      "metadata": {
        "id": "LIGEVtIv5UbL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Package Imports"
      ],
      "metadata": {
        "id": "o8AqL8l4NfUU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from numpy import mean\n",
        "import os\n",
        "import random\n",
        "import pickle\n",
        "import pandas as pd\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import torchvision\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "\n",
        "import torchvision\n",
        "import torchvision.datasets as datasets\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "from tqdm import tqdm\n",
        "\n",
        "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, roc_auc_score\n",
        "from sklearn.model_selection import KFold\n",
        "\n",
        "import gdown"
      ],
      "metadata": {
        "id": "yu61Jp1xrnKk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##  Data\n",
        "The data for this project was obtained from the Alzheimer's Disease Neuroimaging Initiative (ADNI) database. The original experiment used a sample of 358 subjects, which included both control and Alzheimer's Disease patients. The mean age was 77 +/- 7 years. Scans were acquired each year, up to 10 years, for each patient.\n",
        "\n",
        "The GitHub repository contains a folder with partitions used for five fold cross validation with training, validation, and test sets. For training, all longitudinal scans of a particular patient were used. For validation and testing, only a single randomly selected scan was used.\n",
        "\n",
        "I only used a subset of the data since each patient had multiple photos, which would make the total number of photos too high for this project.\n",
        "\n",
        "## Download instructions\n",
        " 1. Request access to the ADNI dataset (https://ida.loni.usc.edu/collaboration/access/appLicense.jsp)\n",
        " 2. Once you receive an email confirming your access, login with your credentials here: https://ida.loni.usc.edu/login.jsp?project=ADNIhttps://ida.loni.usc.edu/login.jsp?project=ADNI\n",
        " 3. Go to Download > Image Collections > Advanced Search\n",
        " 4. Copy and paste the list of subjects in \"subjects.txt\" (found in my GitHub repository) in the \"Subject ID\" field.\n",
        " 5. In the \"Research Group\" section, select AD.\n",
        " 6. In the \"Study/Visit\" section, select all the checkboxes that match \"ADNI1/GO Month X,\" where X is a number.\n",
        " 7. Check off \"T1\" in the \"Weighting\" section.\n",
        " 8. Click \"Search.\"\n",
        " 9. Check \"Select All\" and click \"Add to Collection.\"\n",
        " 10. Enter a name for your collection and click \"OK.\"\n",
        " 11. Go to Data Collections.\n",
        " 12. Click \"My Collection\" > *name of your collection* > \"Not Downloaded\"\n",
        " 13. Check the \"All\" box.\n",
        " 14. Click \"1-Click Download\"\n",
        " 15. Click \"Zip File 1\" to download a zip file of the data.\n",
        " 16. Unzip the file.\n",
        " 17. Convert the images to JPG (see https://medium.com/@vivek8981/dicom-to-jpg-and-extract-all-patients-information-using-python-5e6dd1f1a07d for instructions.)\n",
        " 18. Create a folder called \"data\", which will hold your JPG files, in Google Drive.\n",
        " 19. Within that folder, create a folder called \"train\".\n",
        " 20. Create a folder called \"AD\" within the \"train\" folder and move the images there.\n",
        " 21. Repeat steps 3-17, this time with \"CN\" data (i.e., check the \"CN\" box in the \"Research Group\" section and create a folder called \"CN\" within your \"train\" folder in Google Drive that holds these images.)\n",
        " 22. Create an empty \"val\" folder with empty \"AD\" and \"CN\" folders.\n",
        " 23. In the notebook, change the variable called \"root\" to the path of your folder that includes the \"data\" folder (for instance, mine is '/content/drive/MyDrive/CS_598_Final_Project')\n"
      ],
      "metadata": {
        "id": "2NbPHUTMbkD3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Preprocessing"
      ],
      "metadata": {
        "id": "_kopyaWl5yEK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The paper states that \"All longitudinal images per subject were used for training, whereas a single image, randomly selected from the longitudinal set, was selected for each validation and test patient.\"\n",
        "\n",
        "To create the data loaders, I used all images for the training set, and randomly selected an image for each patient to create the validation set"
      ],
      "metadata": {
        "id": "CaBb-moC273q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Create validation dataset"
      ],
      "metadata": {
        "id": "gXomEsM4b0AO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "sE1Urzkyeurh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO: change this to the name of your folder\n",
        "root = '/content/drive/MyDrive/CS_598_Final_Project'\n",
        "\n",
        "def create_validation_folder(root, p_type):\n",
        "  files = os.listdir(os.fsencode(root + '/train/' + p_type))\n",
        "  patients = set(list(map(lambda file: file[0:15], files)))\n",
        "\n",
        "  for patient in patients:\n",
        "    p = patient.decode()\n",
        "    images = !ls $root/train/$p_type/$p*\n",
        "    selected = random.sample(images, 1)[0]\n",
        "    val_path = root + '/val/' + p_type\n",
        "\n",
        "    !cp $selected $val_path\n",
        "\n",
        "# create_validation_folder(root, 'AD')\n",
        "# create_validation_folder(root, 'CN')"
      ],
      "metadata": {
        "id": "OaoL_Sj8uLJ4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Load data"
      ],
      "metadata": {
        "id": "W9NT5xUyb2lT"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BZScZNbROw-N"
      },
      "outputs": [],
      "source": [
        "def load_data(root_path):\n",
        "    t = transforms.Compose([\n",
        "        transforms.RandomResizedCrop(size=(244, 244)),\n",
        "        transforms.ToTensor()\n",
        "    ])\n",
        "\n",
        "    train_folder = torchvision.datasets.ImageFolder(\n",
        "        root=root_path + '/train',\n",
        "        transform=t\n",
        "    )\n",
        "\n",
        "    val_folder = torchvision.datasets.ImageFolder(\n",
        "        root=root_path + '/val',\n",
        "        transform=t\n",
        "    )\n",
        "\n",
        "    train_loader = torch.utils.data.dataloader.DataLoader(\n",
        "        train_folder,\n",
        "        batch_size=32,\n",
        "        shuffle=True,\n",
        "    )\n",
        "\n",
        "    val_loader = torch.utils.data.dataloader.DataLoader(\n",
        "        val_folder,\n",
        "        batch_size=32,\n",
        "        shuffle=False\n",
        "    )\n",
        "\n",
        "    return train_loader, val_loader\n",
        "\n",
        "def get_stats(root):\n",
        "    number_cn = !ls $root/CN | wc -l\n",
        "    number_ad = !ls $root/AD | wc -l\n",
        "\n",
        "    return int(number_cn[0]), int(number_ad[0])\n",
        "\n",
        "train_loader, val_loader = load_data(root + '/data')\n",
        "print('Train stats: ' + str(get_stats(root + '/data/train')))\n",
        "print('Val stats: ' + str(get_stats(root + '/data/val')))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Statistics"
      ],
      "metadata": {
        "id": "WoG4Ywc1b4MY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The training set includes 887 CN (control) images and 691 AD (Alzheimer's Disease) patients. The validation set includes 15 CN images and 14 AD images."
      ],
      "metadata": {
        "id": "FsLh1vO7blcJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Display images"
      ],
      "metadata": {
        "id": "PQD9eEp_b5Yd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Once you have your train loader, display the images along with their labels using these methods (taken from HW3 Convolutional Neural Network). I cannot show the images to others unless they also have access to the dataset."
      ],
      "metadata": {
        "id": "0-Zl0si9Tu2N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Source: Homework 3: CNNs\n",
        "def imshow(img, title):\n",
        "    npimg = img.numpy()\n",
        "    plt.figure(figsize=(10, 10))\n",
        "    plt.axis('off')\n",
        "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
        "    plt.title(title)\n",
        "    plt.show()\n",
        "\n",
        "def show_batch_images(dataloader, k=8):\n",
        "    images, labels = next(iter(dataloader))\n",
        "    images = images[:k]\n",
        "    labels = labels[:k]\n",
        "    img = torchvision.utils.make_grid(images, padding=25)\n",
        "    imshow(img, title=[\"AD\" if x==0  else \"CN\" for x in labels])\n",
        "\n",
        "'''\n",
        "for i in range(2):\n",
        "    show_batch_images(train_loader)\n",
        "'''"
      ],
      "metadata": {
        "id": "BrpoaSgob8Qy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model"
      ],
      "metadata": {
        "id": "3muyDPFPbozY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Original Paper\n",
        "\"Back to the Basics with Inclusion of Clinical Domain Knowledge - A Simple, Scalable and Effective Model of Alzheimer’s Disease Classification\" (Sarah C. Brüningk, Felix Hensel, Louis P. Lukas, Merel Kuijs, Catherine R. Jutzeler, Bastian Rieck Proceedings of the 6th Machine Learning for Healthcare Conference, PMLR 149:730-754, 2021.)"
      ],
      "metadata": {
        "id": "DfN69SaW6sSQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Original Repository\n",
        "https://github.com/BorgwardtLab/ADNI_3DCNNvsTDA/blob/main"
      ],
      "metadata": {
        "id": "4gx9n0Wg6zlC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Model Descriptions\n",
        "I decided not to use the original repository code for this project. Instead, I created my own simple CNN model, using Homework 3 as a guide. In the paper, the authors said they used batch normalization, dropout, and ReLU activation for image-based classification, so I used these functions, as well."
      ],
      "metadata": {
        "id": "0SqPmzmDEEOC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(CNN, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 6, 5)\n",
        "        self.bn1 = nn.BatchNorm2d(6)\n",
        "        self.act = nn.ReLU()\n",
        "        self.pool = nn.MaxPool2d(2, 2)\n",
        "        self.out = nn.Dropout()\n",
        "        self.fc1 = nn.Linear(86400, 64)\n",
        "        self.fc2 = nn.Linear(64, 32)\n",
        "        self.fc3 = nn.Linear(32, 2)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = self.bn1(x)\n",
        "        x = self.act(x)\n",
        "        x = self.pool(x)\n",
        "        x = self.out(x)\n",
        "        x = torch.flatten(x, 1)\n",
        "        x = self.act(self.fc1(x))\n",
        "        x = self.act(self.fc2(x))\n",
        "        x = self.fc3(x)\n",
        "        return x\n",
        "\n",
        "model = CNN()"
      ],
      "metadata": {
        "id": "OmoET7JlcFxu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training"
      ],
      "metadata": {
        "id": "9z6oeWrMgUOr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)"
      ],
      "metadata": {
        "id": "8IyiWCc981M1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hyperparameters\n",
        "The hyperparameters I chose are\n",
        "- Learning rate (0.0001)\n",
        "- Batch size (32)\n",
        "- Dropout\n"
      ],
      "metadata": {
        "id": "fBm3ej4l3TH3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Computational Requirements\n",
        "- **Number of epochs**: the authors used 2500, but I decided to use 10\n",
        "- **Average runtime per epoch**: 1.36 minute\n",
        "- **Type of hardware**: CPU"
      ],
      "metadata": {
        "id": "GRoS4kbe8Wc_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Training Code"
      ],
      "metadata": {
        "id": "eOrx4_XR8f0E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "criterion = torch.nn.modules.loss.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(params=model.parameters(), lr=0.0001)\n",
        "\n",
        "n = 10\n",
        "\n",
        "def train_model(model, train_loader, n, optimizer, criterion):\n",
        "    model.train()\n",
        "\n",
        "    for epoch in range(n):\n",
        "        for data, target in tqdm(train_loader):\n",
        "            outputs = model(data)\n",
        "            loss = criterion(outputs, target)\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "    return model\n",
        "\n",
        "seed = 42\n",
        "random.seed(seed)\n",
        "np.random.seed(seed)\n",
        "torch.manual_seed(seed)\n",
        "\n",
        "# Evaluate model and save state\n",
        "'''\n",
        "model = train_model(model, train_loader, n, optimizer, criterion)\n",
        "torch.save(dict(model.state_dict()), root + '/model.pth')\n",
        "'''"
      ],
      "metadata": {
        "id": "it5DTAxTeeAq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Evaluation"
      ],
      "metadata": {
        "id": "u5oUY1g1l_Xs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Code"
      ],
      "metadata": {
        "id": "zJ3qO15-jH7j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate(model, dataloader):\n",
        "    model.eval()\n",
        "    Y_pred = []\n",
        "    Y_true = []\n",
        "\n",
        "    for data, target in dataloader:\n",
        "        pred = model(data).argmax(1).detach().numpy()\n",
        "        true = list(target.detach().numpy())\n",
        "\n",
        "        Y_pred.append(pred)\n",
        "        Y_true.append(true)\n",
        "\n",
        "    Y_pred = np.concatenate(Y_pred, axis=0)\n",
        "    Y_true = np.concatenate(Y_true, axis=0)\n",
        "\n",
        "    return Y_pred, Y_true\n",
        "\n",
        "def load_saved_model(url, output, load, c):\n",
        "  gdown.download(url, output)\n",
        "\n",
        "  if c == 0:\n",
        "    loaded_model = CNN()\n",
        "  elif c == 1:\n",
        "    loaded_model = Ablation1CNN()\n",
        "  elif c == 2:\n",
        "    loaded_model = Ablation2CNN()\n",
        "  else:\n",
        "    loaded_model = Ablation3CNN()\n",
        "\n",
        "  loaded_model.load_state_dict(torch.load(load))\n",
        "  return loaded_model\n",
        "\n",
        "def get_metrics(model, val_loader):\n",
        "  y_pred, y_true = evaluate(model, val_loader)\n",
        "\n",
        "  acc = accuracy_score(y_true, y_pred)\n",
        "  f1 = f1_score(y_true, y_pred)\n",
        "  precision = precision_score(y_true, y_pred)\n",
        "  recall = recall_score(y_true, y_pred)\n",
        "  roc_auc = roc_auc_score(y_true, y_pred)\n",
        "\n",
        "  data = {\n",
        "    'Metric': ['Accuracy', 'F1', 'Precision', 'Recall', 'ROC AUC'],\n",
        "    'Result': [acc, f1, precision, recall, roc_auc]\n",
        "  }\n",
        "\n",
        "  df = pd.DataFrame(data=data)\n",
        "  return df\n",
        "\n",
        "url = 'https://drive.google.com/uc?id=1-ojSIoDlQuVm-KEmY_pWtyaGkFJzTACH'\n",
        "output = 'nvswamy2-model'\n",
        "load = '/content/nvswamy2-model'\n",
        "loaded_model = load_saved_model(url, output, load, 0)"
      ],
      "metadata": {
        "id": "P8KSlIlBjJTp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Results"
      ],
      "metadata": {
        "id": "8EAWAy_LwHlV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Since I used a much simpler model and technique than the one described in the paper, my numbers were different than the authors', who \"initially used the full-resolution inner brain volume as input, then evaluated geometric brain subregions (image patches), and finally, brain functional subunits (left and right hippocampus). For each of these image subsets both a TDA and\n",
        "purely image domain-based model was evaluated\" (8). They also  used simple multilayer 3D CNNs with batch normalization, dropout, and ReLU activation for image-based classification. They then used convolutional layers and global\n",
        "average pooling and two dense layers prior to the output layer. They also used four-layer 2D CNNs for persistence image inputs. Lastly, they used a simple GNN for patch images.\n",
        "\n",
        "To evaluate the performance of their model, they used five-fold cross validation and split patients into test/test sets with an 80%-20% split. Within the training sets, they used a 75%-25% split to obtain the final training and validation sets.\n",
        "\n",
        "They found that for the 3D images, their model had an accuracy and AUC of 0.79 +/ 0.05 and 0.88 +/- 0.05, respectively. When analyzing 3D image patches, their highest accuracy and AUC were 0.81 +/ 0.05 and 0.89 +/- 0.05, respectively (these numbers were observed if the patch was close to the hippocampus.) They also evaluated a 3D CNN model on the left and right hippocampus individually and found that the model on the left yieled better results than the right (0.84 +/ 0.07 accuracy and 0.91 +/- 0.05 AUC vs. 0.80 +/- 0.08 accuracy and 0.88 +/- 0.06 AUC).\n",
        "\n",
        "In my version, my results are below:"
      ],
      "metadata": {
        "id": "EswVreE3hbFj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "get_metrics(loaded_model, val_loader)"
      ],
      "metadata": {
        "id": "kGJMJ6RIw3Ip"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "My accuracy and AUC (0.51, 0.5, respectively) are much lower than the results from the paper."
      ],
      "metadata": {
        "id": "NaZVoKzILIyC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Ablation Study"
      ],
      "metadata": {
        "id": "ot7IxftP98HQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "For the ablation study, I decided to remove the certain components from the CNN model to see how they impact performance."
      ],
      "metadata": {
        "id": "m7FMuWeuzf9N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Removing Batch Normalization"
      ],
      "metadata": {
        "id": "i4t_0-FhC_f8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Ablation1CNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Ablation1CNN, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 6, 5)\n",
        "        self.act = nn.ReLU()\n",
        "        self.pool = nn.MaxPool2d(2, 2)\n",
        "        self.out = nn.Dropout()\n",
        "        self.fc1 = nn.Linear(86400, 64)\n",
        "        self.fc2 = nn.Linear(64, 32)\n",
        "        self.fc3 = nn.Linear(32, 2)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = self.act(x)\n",
        "        x = self.pool(x)\n",
        "        x = self.out(x)\n",
        "        x = torch.flatten(x, 1)\n",
        "        x = self.act(self.fc1(x))\n",
        "        x = self.act(self.fc2(x))\n",
        "        x = self.fc3(x)\n",
        "        return x\n",
        "\n",
        "'''\n",
        "ablation_1_model = Ablation1CNN()\n",
        "optimizer = torch.optim.Adam(params=ablation_1_model.parameters(), lr=0.0001)\n",
        "ablation_1_model = train_model(ablation_1_model, train_loader, n, optimizer, criterion)\n",
        "torch.save(dict(ablation_1_model.state_dict()), root + '/ablation-1-model.pth')\n",
        "'''\n",
        "\n",
        "url = 'https://drive.google.com/uc?id=1--NtUimicL534BDfWG789KH2AXGyygre'\n",
        "output = 'nvswamy2-ablation-1-model'\n",
        "load = '/content/nvswamy2-ablation-1-model'\n",
        "\n",
        "loaded_ablation_model = load_saved_model(url, output, load, 1)\n",
        "loaded_ablation_model.load_state_dict(torch.load(load))\n",
        "\n",
        "get_metrics(loaded_ablation_model, val_loader)"
      ],
      "metadata": {
        "id": "XSVVfotAza_k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Removing ReLU Activation"
      ],
      "metadata": {
        "id": "T8izOKaEDlp7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Ablation2CNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Ablation2CNN, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 6, 5)\n",
        "        self.bn1 = nn.BatchNorm2d(6)\n",
        "        self.pool = nn.MaxPool2d(2, 2)\n",
        "        self.out = nn.Dropout()\n",
        "        self.fc1 = nn.Linear(86400, 64)\n",
        "        self.fc2 = nn.Linear(64, 32)\n",
        "        self.fc3 = nn.Linear(32, 2)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = self.bn1(x)\n",
        "        x = self.pool(x)\n",
        "        x = self.out(x)\n",
        "        x = torch.flatten(x, 1)\n",
        "        x = self.fc1(x)\n",
        "        x = self.fc2(x)\n",
        "        x = self.fc3(x)\n",
        "        return x\n",
        "\n",
        "'''\n",
        "ablation_2_model = Ablation2CNN()\n",
        "optimizer = torch.optim.Adam(params=ablation_2_model.parameters(), lr=0.0001)\n",
        "ablation_2_model = train_model(ablation_2_model, train_loader, n, optimizer, criterion)\n",
        "torch.save(dict(ablation_2_model.state_dict()), root + '/ablation-2-model.pth')\n",
        "'''\n",
        "\n",
        "url = 'https://drive.google.com/uc?id=1-4Aop_-iOSCUju39TGtRt7U8V8eAhPeg'\n",
        "output = 'nvswamy2-ablation-2-model'\n",
        "load = '/content/nvswamy2-ablation-2-model'\n",
        "\n",
        "loaded_ablation_model = load_saved_model(url, output, load, 2)\n",
        "loaded_ablation_model.load_state_dict(torch.load(load))\n",
        "\n",
        "get_metrics(loaded_ablation_model, val_loader)"
      ],
      "metadata": {
        "id": "R8oSOk5y0QOJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Removing Dropout"
      ],
      "metadata": {
        "id": "ftObryiNEF_x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Ablation3CNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Ablation3CNN, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 6, 5)\n",
        "        self.bn1 = nn.BatchNorm2d(6)\n",
        "        self.act = nn.ReLU()\n",
        "        self.pool = nn.MaxPool2d(2, 2)\n",
        "        self.fc1 = nn.Linear(86400, 64)\n",
        "        self.fc2 = nn.Linear(64, 32)\n",
        "        self.fc3 = nn.Linear(32, 2)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = self.bn1(x)\n",
        "        x = self.act(x)\n",
        "        x = self.pool(x)\n",
        "        x = torch.flatten(x, 1)\n",
        "        x = self.act(self.fc1(x))\n",
        "        x = self.act(self.fc2(x))\n",
        "        x = self.fc3(x)\n",
        "        return x\n",
        "\n",
        "'''\n",
        "ablation_3_model = Ablation3CNN()\n",
        "optimizer = torch.optim.Adam(params=ablation_3_model.parameters(), lr=0.0001)\n",
        "ablation_3_model = train_model(ablation_3_model, train_loader, n, optimizer, criterion)\n",
        "torch.save(dict(ablation_3_model.state_dict()), root + '/ablation-3-model.pth')\n",
        "'''\n",
        "\n",
        "url = 'https://drive.google.com/uc?id=1-4H-M19qHQMPCODpftXThAJQ5bKYJZQT'\n",
        "output = 'nvswamy2-ablation-3-model'\n",
        "load = '/content/nvswamy2-ablation-3-model'\n",
        "\n",
        "loaded_ablation_model = load_saved_model(url, output, load, 3)\n",
        "loaded_ablation_model.load_state_dict(torch.load(load))\n",
        "\n",
        "get_metrics(loaded_ablation_model, val_loader)"
      ],
      "metadata": {
        "id": "sq8VBTX5EFgx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Based on the results, it appears the the ReLU activation had the most impact since each metric increased once removing it, while removing batch normalization and dropout had almost no impact."
      ],
      "metadata": {
        "id": "FhI4bQahQxBa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Discussion\n",
        "\n",
        "This project was difficult to duplicate for a couple of reasons.\n",
        "1. While the original paper discusses the origin of the data and the metrics (sample size, mean age, etc.), they do not provide instructions on which specific images they used and how they navigated the ADNI database. I was able to find a list of subjects used from the original repository, so I downloaded images from a subset of those subjects.\n",
        "2. The original code was quite complicated for me, as I have limited experience in working with neural networks and PyTorch. However, I understand that this is a skill in which I can improve and not entirely the fault of the authors.\n",
        "\n",
        "I think the authors could improve their project by having a more through README with instructions on how to access the data and how to navigate the codebase. Comments throughout the code files would have been very helpful, as well.\n",
        "\n",
        "If I were to improve my version of the project in the future, I would perform experiments on 3D images. I would also take patches of images and see how the model compares on the left versus the right hippocampus as the authors did.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "qH75TNU71eRH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Public GitHub Link\n",
        "https://github.com/nehaswamy/cs598-final-project/tree/main\n"
      ],
      "metadata": {
        "id": "SHMI2chl9omn"
      }
    }
  ]
}