{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MQ0sNuMePBXx"
   },
   "source": [
    "# Introduction\n",
    "This project is based on \"Back to the Basics with Inclusion of Clinical Domain Knowledge - A Simple, Scalable and Effective Model of Alzheimer’s Disease Classification\" (Sarah C. Brüningk, Felix Hensel, Louis P. Lukas, Merel Kuijs, Catherine R. Jutzeler, Bastian Rieck Proceedings of the 6th Machine Learning for Healthcare Conference, PMLR 149:730-754, 2021.)\n",
    "\n",
    "The authors of the original paper analyze MRIs of patients with Alzheimer's disease to detect the onset of the disease early, predict disease progression, and optimize treatment plans for patients.\n",
    "\n",
    "Currently, there is a push for more complex machine learning models, like deep CNNs with unsupervised learning and ensemble models that combine different architectures. Complex models may perform well, but they are not necessarily applicable in clinical settings because high-resolution MRIs lead to memory and hardware issues. One method used to achieve high performance is down-sampling data, but this means that high-resolution data that could be useful is scrapped.\n",
    "\n",
    "The authors hypothesize that using simple models that include clinical domain knowledge of Alzheimer's disease and its topological features will result in a classification performance similar to that of complex models while also being faster, more scalable, and more interpretable. They evaluated the performance of CNNs on subsets of full-resolution three-dimensional MRIs. Then, they performed an ablation study to compare different levels of biological scale and anatomical brain areas. Finally, they used topological data analysis (TDA) to analyze changes in whole brain tissue connectivity.\n",
    "\n",
    "They found that choosing a meaningful data representation consisting of the left hippocampus allowed them to achieve better performance compared to more complex architectures with many parameters. In addition, they were able to save on computational cost and hardware requirements. This means that clinical domain knowledge may be more important than architecture design and complexity, specifically for Alzheimer's Disease classification."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6od6tHp2gHGp"
   },
   "source": [
    "# Public GitHub Link\n",
    "https://github.com/nehaswamy/cs598-final-project/tree/main"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uygL9tTPSVHB"
   },
   "source": [
    "# Scope of Reproducibility:\n",
    "\n",
    "The authors' hypotheses were\n",
    "1. \"...using simple model architectures that include prior domain knowledge of morphological hallmarks of [Alzheimer's Disease] and topological feature thereof, will result in classification performance comparable to more complex model[s]\" (3), and\n",
    "2. \"...[their] models will facilitate fast, scalable, and interpretable solutions through meaningful representations\" (3).\n",
    "\n",
    "In other words, they sought to determine if simple models are faster, more usable, and more understandable than highly complex models with too many hyperparameters.\n",
    "\n",
    "In this project, I use a simple CNN model, using HW 3: CNNs as a guide, to test if a basic CNN will have a high accuracy (>= 70%) and perform fast."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "87eMgS0JPWQB"
   },
   "source": [
    "# Steps to Access and Load the Data\n",
    "1. Request access to the ADNI dataset (https://ida.loni.usc.edu/collaboration/access/appLicense.jsp)\n",
    "2. Once you receive an email confirming your access, login with your credentials here: https://ida.loni.usc.edu/login.jsp?project=ADNIhttps://ida.loni.usc.edu/login.jsp?project=ADNI\n",
    "3. Go to Download > Image Collections > Advanced Search\n",
    "4. Copy and paste the list of subjects in \"subjects.txt\" (found in my GitHub repository) in the \"Subject ID\" field.\n",
    "5. In the \"Research Group\" section, select AD.\n",
    "6. In the \"Study/Visit\" section, select all the checkboxes that match \"ADNI1/GO Month X,\" where X is a number.\n",
    "7. Check off \"T1\" in the \"Weighting\" section.\n",
    "8. Click \"Search.\"\n",
    "9. Check \"Select All\" and click \"Add to Collection.\"\n",
    "10. Enter a name for your collection and click \"OK.\"\n",
    "11. Go to Data Collections.\n",
    "12. Click \"My Collection\" > *name of your collection* > \"Not Downloaded\"\n",
    "13. Check the \"All\" box.\n",
    "14. Click \"1-Click Download\"\n",
    "15. Click \"Zip File 1\" to download a zip file of the data.\n",
    "16. Unzip the file.\n",
    "17. Convert the images to JPG (see https://medium.com/@vivek8981/dicom-to-jpg-and-extract-all-patients-information-using-python-5e6dd1f1a07d for instructions.)\n",
    "18. Create a folder called \"data\", which will hold your JPG files, in Google Drive.\n",
    "19. Within that folder, create a folder called \"train\".\n",
    "20. Create a folder called \"AD\" within the \"train\" folder and move the images there.\n",
    "21. Repeat steps 3-17, this time with \"CN\" data (i.e., check the \"CN\" box in the \"Research Group\" section and create a folder called \"CN\" within your \"train\" folder in Google Drive that holds these images.)\n",
    "22. Create an empty \"val\" folder with empty \"AD\" and \"CN\" folders.\n",
    "23. In the notebook, change the variable called \"root\" to the path of your folder that includes the \"data\" folder (for instance, mine is '/content/drive/MyDrive/CS_598_Final_Project')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xWAHJ_1CdtaA"
   },
   "source": [
    "# Methodology"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o8AqL8l4NfUU"
   },
   "source": [
    "## Package Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yu61Jp1xrnKk"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy import mean\n",
    "import os\n",
    "import random\n",
    "import pickle\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torchvision\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "import torchvision\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "\n",
    "import gdown"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2NbPHUTMbkD3"
   },
   "source": [
    "##  Data\n",
    "The data for this project was obtained from the Alzheimer's Disease Neuroimaging Initiative (ADNI) database. The original experiment used a sample of 358 subjects, which included both control and Alzheimer's Disease patients. The mean age was 77 +/- 7 years. Scans were acquired each year, up to 10 years, for each patient.\n",
    "\n",
    "The GitHub repository contains a folder with partitions used for five fold cross validation with training, validation, and test sets. For training, all longitudinal scans of a particular patient were used. For validation and testing, only a single randomly selected scan was used.\n",
    "\n",
    "I only used a subset of the data since each patient had multiple photos, which would make the total number of photos too high for this project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "moGZNfLkpUIq"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CaBb-moC273q"
   },
   "source": [
    "The paper states that \"All longitudinal images per subject were used for training, whereas a single image, randomly selected from the longitudinal set, was selected for each validation and test patient.\"\n",
    "\n",
    "To create the data loaders, I used all images for the training set, and randomly selected an image for each patient to create the validation set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gXomEsM4b0AO"
   },
   "source": [
    "### Create validation dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OaoL_Sj8uLJ4"
   },
   "outputs": [],
   "source": [
    "# TODO: change this to the name of your folder\n",
    "root = '/content/drive/MyDrive/CS_598_Final_Project'\n",
    "\n",
    "def create_validation_folder(root, p_type):\n",
    "  files = os.listdir(os.fsencode(root + '/train/' + p_type))\n",
    "  patients = set(list(map(lambda file: file[0:15], files)))\n",
    "\n",
    "  for patient in patients:\n",
    "    p = patient.decode()\n",
    "    images = !ls $root/train/$p_type/$p*\n",
    "    selected = random.sample(images, 1)[0]\n",
    "    val_path = root + '/val/' + p_type\n",
    "\n",
    "    !cp $selected $val_path\n",
    "\n",
    "create_validation_folder(root, 'AD')\n",
    "create_validation_folder(root, 'CN')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W9NT5xUyb2lT"
   },
   "source": [
    "### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BZScZNbROw-N"
   },
   "outputs": [],
   "source": [
    "def load_data(root_path):\n",
    "    t = transforms.Compose([\n",
    "        transforms.RandomResizedCrop(size=(244, 244)),\n",
    "        transforms.ToTensor()\n",
    "    ])\n",
    "\n",
    "    train_folder = torchvision.datasets.ImageFolder(\n",
    "        root=root_path + '/train',\n",
    "        transform=t\n",
    "    )\n",
    "\n",
    "    val_folder = torchvision.datasets.ImageFolder(\n",
    "        root=root_path + '/val',\n",
    "        transform=t\n",
    "    )\n",
    "\n",
    "    train_loader = torch.utils.data.dataloader.DataLoader(\n",
    "        train_folder,\n",
    "        batch_size=32,\n",
    "        shuffle=True,\n",
    "    )\n",
    "\n",
    "    val_loader = torch.utils.data.dataloader.DataLoader(\n",
    "        val_folder,\n",
    "        batch_size=32,\n",
    "        shuffle=False\n",
    "    )\n",
    "\n",
    "    return train_loader, val_loader\n",
    "\n",
    "def get_stats(root):\n",
    "    number_cn = !ls $root/CN | wc -l\n",
    "    number_ad = !ls $root/AD | wc -l\n",
    "\n",
    "    return int(number_cn[0]), int(number_ad[0])\n",
    "\n",
    "train_loader, val_loader = load_data(root + '/data')\n",
    "print('Train stats: ' + str(get_stats(root + '/data/train')))\n",
    "print('Val stats: ' + str(get_stats(root + '/data/val')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WoG4Ywc1b4MY"
   },
   "source": [
    "### Statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FsLh1vO7blcJ"
   },
   "source": [
    "The training set includes 887 CN (control) images and 691 AD (Alzheimer's Disease) patients. The validation set includes 15 CN images and 14 AD images."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PQD9eEp_b5Yd"
   },
   "source": [
    "### Display images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0-Zl0si9Tu2N"
   },
   "source": [
    "Once you have your train loader, display the images along with their labels using these methods (taken from HW3 Convolutional Neural Network)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BrpoaSgob8Qy"
   },
   "outputs": [],
   "source": [
    "# Source: Homework 3: CNNs\n",
    "def imshow(img, title):\n",
    "    npimg = img.numpy()\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    plt.axis('off')\n",
    "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
    "    plt.title(title)\n",
    "    plt.show()\n",
    "\n",
    "def show_batch_images(dataloader, k=8):\n",
    "    images, labels = next(iter(dataloader))\n",
    "    images = images[:k]\n",
    "    labels = labels[:k]\n",
    "    img = torchvision.utils.make_grid(images, padding=25)\n",
    "    imshow(img, title=[\"AD\" if x==0  else \"CN\" for x in labels])\n",
    "\n",
    "for i in range(2):\n",
    "    show_batch_images(train_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3muyDPFPbozY"
   },
   "source": [
    "##   Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "g5rMGh8ogQf1"
   },
   "source": [
    "### Simple CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0SqPmzmDEEOC"
   },
   "source": [
    "Currently, I am reusing the CNN model from homework 3 to get an understanding about how a simple CNN model can be created. However, for the final project, I intend on finetuning this and updating the fields and forward method so that is more applicable to the problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OmoET7JlcFxu"
   },
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 6, 5)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "        self.fc1 = nn.Linear(53824, 64)\n",
    "        self.fc2 = nn.Linear(64, 32)\n",
    "        self.fc3 = nn.Linear(32, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "model = CNN()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9z6oeWrMgUOr"
   },
   "source": [
    "### Train Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fBm3ej4l3TH3"
   },
   "source": [
    "The authors used 2500 epochs. However, for this draft, I used 10."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "it5DTAxTeeAq"
   },
   "outputs": [],
   "source": [
    "criterion = torch.nn.modules.loss.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(params=model.parameters(), lr=0.0001)\n",
    "\n",
    "n = 10\n",
    "\n",
    "def train_model(model, train_loader, n, optimizer, criterion):\n",
    "    model.train()\n",
    "\n",
    "    for epoch in range(n):\n",
    "        for data, target in tqdm(train_loader):\n",
    "            outputs = model(data)\n",
    "            loss = criterion(outputs, target)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "    return model\n",
    "\n",
    "seed = 42\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "\n",
    "# Evaluate model and save state\n",
    "'''\n",
    "model = train_model(model, train_loader, n, optimizer, criterion)\n",
    "torch.save(dict(model.state_dict()), root + '/model.pth')\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u5oUY1g1l_Xs"
   },
   "source": [
    "# Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zJ3qO15-jH7j"
   },
   "source": [
    "### Evaluate Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "P8KSlIlBjJTp"
   },
   "outputs": [],
   "source": [
    "def evaluate(model, dataloader):\n",
    "    model.eval()\n",
    "    Y_pred = []\n",
    "    Y_true = []\n",
    "\n",
    "    for data, target in dataloader:\n",
    "        pred = model(data).argmax(1).detach().numpy()\n",
    "        true = list(target.detach().numpy())\n",
    "\n",
    "        Y_pred.append(pred)\n",
    "        Y_true.append(true)\n",
    "\n",
    "    Y_pred = np.concatenate(Y_pred, axis=0)\n",
    "    Y_true = np.concatenate(Y_true, axis=0)\n",
    "\n",
    "    return Y_pred, Y_true\n",
    "\n",
    "url = 'https://drive.google.com/uc?id=1-ojSIoDlQuVm-KEmY_pWtyaGkFJzTACH'\n",
    "output = 'nvswamy2-model'\n",
    "gdown.download(url, output)\n",
    "\n",
    "loaded_model = CNN()\n",
    "loaded_model.load_state_dict(torch.load('/content/nvswamy2-model'))\n",
    "\n",
    "y_pred, y_true = evaluate(loaded_model, val_loader)\n",
    "acc = accuracy_score(y_true, y_pred)\n",
    "f1 = f1_score(y_true, y_pred)\n",
    "\n",
    "print('Accuracy: ' + str(acc))\n",
    "print('F1 Score: ', str(f1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8EAWAy_LwHlV"
   },
   "source": [
    "## Model Comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EswVreE3hbFj"
   },
   "source": [
    "Since I used a much simpler model and technique than the one described in the paper, my numbers were different than the authors', who \"initially used the full-resolution inner brain volume as input, then evaluated geometric brain subregions (image patches), and finally, brain functional subunits (left and right hippocampus). For each of these image subsets both a TDA and\n",
    "purely image domain-based model was evaluated\" (8). They also  used simple multilayer 3D CNNs with batch normalization, dropout, and ReLU activation for image-based classification. They then used convolutional layers and global\n",
    "average pooling and two dense layers prior to the output layer. They also used four-layer 2D CNNs for persistence image inputs. Lastly, they used a simple GNN for patch images.\n",
    "\n",
    "To evaluate the performance of their model, they used five-fold cross validation and split patients into test/test sets with an 80%-20% split. Within the training sets, they used a 75%-25% split to obtain the final training and validation sets.\n",
    "\n",
    "They found that for the 3D images, their model had an accuracy and AUC of 0.79 +/ 0.05 and 0.88 +/- 0.05, respectively. When analyzing 3D image patches, their highest accuracy and AUC were 0.81 +/ 0.05 and 0.89 +/- 0.05, respectively (these numbers were observed if the patch was close to the hippocampus.) They also evaluated a 3D CNN model on the left and right hippocampus individually and found that the model on the left yieled better results than the right (0.84 +/ 0.07 accuracy and 0.91 +/- 0.05 AUC vs. 0.80 +/- 0.08 accuracy and 0.88 +/- 0.06 AUC).\n",
    "\n",
    "For this draft, my initial accuracy is 0.51. I plan on finetuning my model and procedure for the final submission.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qH75TNU71eRH"
   },
   "source": [
    "# Discussion\n",
    "\n",
    "This project was difficult to duplicate for a couple of reasons.\n",
    "1. While the original paper discusses the origin of the data and the metrics (sample size, mean age, etc.), they do not provide instructions on which specific images they used and how they navigated the ADNI database. I was able to find a list of subjects used from the original repository, so I downloaded images from a subset of those subjects.\n",
    "2. The original code was quite complicated for me, as I have limited experience in working with neural networks and PyTorch. However, I understand that this is a skill in which I can improve and not entirely the fault of the authors.\n",
    "\n",
    "I think the authors could improve their project by having a more through README with instructions on how to access the data and how to navigate the codebase. Comments throughout the code files would have been very helpful, as well.\n",
    "\n",
    "If I were to improve my version of the project in the future, I would perform experiments on 3D images. I would also take patches of images and see how the model compares on the left versus the right hipppoocampus as the authors did.\n",
    "\n",
    "In the next phase, I will tweak my CNN model and use other measures to test my model's performance, like cross-validation, and include other metrics aside from accuracy and F1 score. I will also create plots to visualize the performance of my model.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SHMI2chl9omn"
   },
   "source": [
    "# References\n",
    "\n",
    "1.   Sarah C. Brüningk, Felix Hensel, Louis P. Lukas, Merel Kuijs, Catherine R. Jutzeler, Bastian Rieck Proceedings of the 6th Machine Learning for Healthcare Conference, PMLR 149:730-754, 2021.\n",
    "2. https://github.com/BorgwardtLab/ADNI_3DCNNvsTDA/blob/main\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "private_outputs": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
